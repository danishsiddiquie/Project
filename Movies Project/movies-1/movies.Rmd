---
title: "movies"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(caret)
library(MASS)
library(ggfortify)
library(dplyr)
library(reshape2)
library(readr)
library(dplyr)
```

# Question 1

```{r}
movies = read_csv("movies.csv")
genomes<-read_csv("MovieGenome.csv")
tags<-read_csv("genome-tags.csv")
ratings<-read_csv("ratings.csv")

```

# Question 2

```{r}
genome.pca = prcomp(genomes[,2:1129], scale.=FALSE,center = FALSE)
```

```{r}
######
##Plotting proportion of variance explained
pca_var = genome.pca$sdev^2
prop_varex <- pca_var/sum(pca_var) #Proportion of variance

plot(cumsum(prop_varex),type='b') #Cumulative proportion of variance

```


```{r}
pca_df<-data.frame(movieId=genomes$movieId,genome.pca$x[,1:500])

```

# Question 3

79357: Mr Nobody (2009)
```{r}

distances<-numeric(10381)

pca_dist<-pca_df[,2:500]


pca_df[match(79357,pca_df$movieId),]

myMovieRow<-pca_dist[9100,]


for(i in 1:10381){
  
  df<-data.frame()
  df = rbind(myMovieRow,pca_dist[i,])
  d=dist(df,method="euclidian")
  
  distances[i]=d
}


```

```{r}
dist_df<-data.frame(movieId=pca_df$movieId,distances)

dist_df<-dist_df[order(dist_df$distances),]

top3<-dist_df[2:4,]
bottom3<-dist_df[10379:10381,]
```

```{r}
movies[match(53464,movies$movieId),]
```


Top 3 movies:

1) id 48043: Fountain,The (2006)
2) id 4975: Vanilla Sky (1982)
3) id 89039: Another Earth (2011)

Bottom 3 movies:

1) id 5254:  Blade II (2002)
2) id 51007: Days of Glory (IndigÃ¨nes) (2006)
3) id 53464: Fantastic Four: Rise of the Silver Surfer (2007)


#Question 4

Movie id of Mr. Nobody: 79357
```{r}
movieRatings<-ratings[which(ratings$movieId==79357),]
myRatingsCsv<-write.csv(movieRatings,"movieRatings.csv")
```

```{r}
myRatings<-read.csv("movieRatings.csv")

```

#Question 5

Global Average 

```{r}
global_avg_rating<-sum(myRatings$rating)/973
each_userDf<-ratings%>%group_by(userId)%>%summarize(avg_rating=sum(rating)/n())
global_avg_rating
```


User Average 

```{r}

myUser_ratings<-data.frame()
for(i in 1:973){
  
  userId = myRatings$userId[i]
  user_movies<-ratings[which(ratings$userId==userId),]
  
  myUser_ratings<-rbind(myUser_ratings,user_movies)
}

myUser_ratings<-myUser_ratings%>%filter(movieId==79357)

each_userDf<-inner_join(myRatings,each_userDf,by="userId")

myUser_ratings<-cbind(myUser_ratings,avg_rating=each_userDf$avg_rating)
```


#Question 6

```{r}
avg_ratings<-ratings%>%group_by(movieId)%>%summarize(rating=sum(rating/n()))

knnDF<-inner_join(dist_df,avg_ratings,by="movieId")

knnDF<-knnDF[2:8474,]

```


Knn Function

```{r}
kEstimate<-function(df,k){
  
  sum_rating<-0
  rating<-numeric(k)
  weights<-df$distances[1:k]
  
  for(i in 1:k){
    
    rate=df$rating[i]
    rating[i] <- rate
    
  }
  avg_rate=mean(rating)
  
  weighted_avg<-weighted.mean(rating,weights)
  
  return(list(avg_rate,weighted_avg))
  
}

kEstimate(knnDF,10)

```

#Question 7

```{r}

bigDf<-data.frame()
for(i in 1:973){
  
  df<-data.frame(id=0,r=0,wr=0)
  
  #id<-numeric(i)
  #r<-numeric(i)
  #wr<-numeric(i)
  
  userId=myRatings$userId[i]
  
  user_movies<-ratings[which(ratings$userId==userId),]
  
  estimate_df<-knnDF[,1:2]
  
  user_rating_DF<-inner_join(estimate_df,user_movies,by="movieId")
  
  for(k in 1:50){
    
    rating=kEstimate(user_rating_DF,k)
    
    #id[k] = userId
    #r[k] = rating[1]
    #wr[k] = rating[2]
    
    id = userId
    r = as.numeric(rating[1])
    wr = as.numeric(rating[2])
    
    df<-rbind(df,c(id,r,wr))
    #df=as.data.frame(df)
    
  }
  bigDf<-rbind(bigDf,df)
  
}

bigDf<-bigDf%>%filter(id!=0)

```


```{r}
csv<-write.csv(bigDf,"bigDf.csv")

```

```{r}
bigDf<-read.csv("bigDf.csv")
bigDf<-bigDf[-c(1)]
```

```{r}

kVals<-data.frame()

for(i in 1:50){
  kVals=rbind(kVals,i)
}

k<-data.frame()

for(i in 1:973){
  k<-rbind(k,kVals)
}

colnames(k)<-"k"

bigDf<-cbind(bigDf,k)

```


#Question 8

```{r}
users_grouped<-bigDf%>%group_by(id=bigDf$id)%>%summarize(avg_r=mean(r),avg_wt_r=mean(wr))

my_movie_r<-numeric(973)
for(i in 1:973){
  
  user_row<-myRatings[match(users_grouped$id[i],myRatings$userId),]
  user_rating = user_row[,4]
  my_movie_r[i]=user_rating
}

users_grouped<-cbind(users_grouped,my_movie_r)


bigDf<-left_join(bigDf,users_grouped,by="id")

bigDf<-bigDf[-c(5:6)]

bigDf<-bigDf%>%mutate(avg_MSE=(r-my_movie_r)^2,
                                      wt_MSE=(wr-my_movie_r)^2)

```

```{r}
k_grouped<-bigDf%>%group_by(k=bigDf$k)%>%summarize(avg_MSE=mean(avg_MSE),wt_MSE=mean(wt_MSE))

#k_grouped<-k_grouped%>%mutate(avg_MSE=(avg_r-my_movie_r)^2,
                                      #wt_MSE=(avg_wt_r-my_movie_r)^2)

```

user average MSE line
```{r}
myUser_ratings<-myUser_ratings%>%mutate(MSE=(avg_rating-rating)^2)

user_MSE<-mean(myUser_ratings$MSE)
```

global average MSE line
```{r}
users_grouped$global_avg<-rep(global_avg_rating,len=973)
users_grouped<-users_grouped%>%mutate(MSE=(global_avg-my_movie_r)^2)

global_MSE<-mean(users_grouped$MSE)
```



```{r}
ggplot()+
  geom_line(aes(k_grouped$k,k_grouped$avg_MSE,col="unweighted knn"))+
  geom_line(aes(k_grouped$k,k_grouped$wt_MSE,col="weighted knn"))+
  geom_line(aes(k_grouped$k,global_MSE,col="movie baseline"))+
  geom_line(aes(k_grouped$k,user_MSE,col="user baseline"))
```


According to the graph, the best k value to use would be k=15, since the MSE value is the lowest at that point. Our predictions from the model seem to be pretty good, since the MSE values at the lowest point are pretty small. The lowest MSE value of 0.7 at k=15 suggests that the model can predict the user's prediction for a test movie as close as +- 0.7 rating value. Hence, if the the user gives a movie a rating of 4.0, our model would have predicted the rating between 3.3 and 4.7. 


#Question 9

Since our model can predict a user's rating of a test movie, we can see check for how the user might predict a particular movie, and if the predicted rating is high, then we should suggest it to the user. For a large scale streaming site, there are millions of viewers who watch movies and the data is stored for them. The model can use the data when a new movies has just arrived on netflix, and can calculate the distances from that movie to the closest movies users have seen and provide predictions for it. The more movies the viewers watch, the better the model can be trained and provide better predictions.

However, for large scale movie streaming sited, the computation has to be faster, both for the model to run and for the recommender system to suggest. One way we would have improved the run time of our model was using the lapply and sapply function instead of nested for-loops to loop over all the users in our data set. Another way to make recommendations computationally fast is to use multiple servers to run the code, in order to divide the workload, making the calculations faster. We can also subset a viewer's data by genre, and compare new movies by genre to predict ratings, so that less memory is occupied at a time. It will however, give a very specific rating, and thus while it can be more accurate for particular genres, it may not be the case for predicting from one genre to the other. 
Finally, we can figure out another, better dimensionality reduction method that can further reduce the data than PCA can. 








